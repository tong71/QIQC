import streamlit as st
import pandas as pd
import numpy as np
from factor_analyzer import FactorAnalyzer, calculate_kmo, calculate_bartlett_sphericity
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns
import umap
st.set_page_config(page_title="é©—è­‰æ¨¡å¼", page_icon="ğŸ§")

st.title("é©—è­‰éšæ®µï¼šè³‡æ–™æª¢é©—èˆ‡ç•°å¸¸å¡«ç­”åµæ¸¬")
st.markdown("""
ä¾ç…§ä¸‹åˆ—æ­¥é©Ÿï¼Œé€²è¡Œå•å·è³‡æ–™é©—è­‰åŠç•°å¸¸å¡«ç­”æª¢æ¸¬ï¼Œæå‡é‡è¡¨ç§‘å­¸æ€§èˆ‡è³‡æ–™å“è³ªã€‚
""")

# ===== Step 1: ä¸Šå‚³èˆ‡è¨­å®š =====
st.header("Step 1. ä¸Šå‚³å•å·è³‡æ–™èˆ‡è¨­å®š")
file = st.file_uploader("è«‹ä¸Šå‚³å•å·ç­”æ¡ˆæª”æ¡ˆ (CSV æˆ– Excel)", type=["csv", "xlsx"])

if file:
    if file.name.endswith(".csv"):
        df = pd.read_csv(file)
    else:
        df = pd.read_excel(file)

    st.write("ä½ ä¸Šå‚³çš„è¡¨æ ¼ï¼š")
    st.dataframe(df, hide_index=True)

    # åªåˆ†æç´”æ•¸å€¼å‹æ¬„ä½
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if not numeric_cols:
        st.error("æœªç™¼ç¾ä»»ä½•ç´”æ•¸å€¼å‹æ¬„ä½ï¼Œè«‹ç¢ºèªä½ çš„å•å·ç­”æ¡ˆæª”æ¡ˆæ ¼å¼æ­£ç¢ºï¼ˆæ‡‰è©²æ¯é¡Œéƒ½æ˜¯æ•¸å€¼åˆ†æ•¸ï¼‰ï¼")
        st.stop()

    question_cols = st.multiselect(
        "è«‹é¸æ“‡è¦åˆ†æçš„å•å·é¡Œç›®æ¬„ä½ï¼ˆåƒ…å¯é¸ç´”æ•¸å€¼å‹æ¬„ä½ï¼‰",
        options=numeric_cols,
        default=numeric_cols[:min(5, len(numeric_cols))]
    )
    if not question_cols or len(question_cols) < 2:
        st.warning("è«‹é¸å–è‡³å°‘å…©å€‹ç´”æ•¸å€¼å‹æ¬„ä½ã€‚")
        st.markdown("#### ç¯„ä¾‹è³‡æ–™æ ¼å¼")
        example = pd.DataFrame({
            "Q1": [1, 2, 4, 5],
            "Q2": [3, 1, 2, 4],
            "Q3": [5, 4, 3, 2],
        })
        st.dataframe(example, hide_index=True)
        st.markdown("""
        <small>
        - æ¯ä¸€æ¬„ä»£è¡¨ä¸€å€‹é¡Œç›®ï¼Œæ¬„ä½åç¨±å¦‚ Q1ã€Q2...<br>
        - æ¯ä¸€åˆ—æ˜¯ä¸€ä»½å•å·ç­”æ¡ˆ<br>
        - æ‰€æœ‰æ¬„ä½éƒ½æ‡‰ç‚ºæ•¸å­—ï¼ˆå¦‚ 1~5ã€1~7 ä¹‹ Likert åˆ†æ•¸ï¼‰<br>
        - è‹¥æ¬„ä½ä¸­æ··æœ‰æ–‡å­—ï¼ˆå¦‚ "éå¸¸åŒæ„"ï¼‰ï¼Œè«‹å…ˆè½‰ç‚ºæ•¸å€¼
        </small>
        """, unsafe_allow_html=True)
        st.stop()


    # è³‡æ–™å‹æ…‹è½‰æ›èˆ‡ç¼ºå¤±å€¼è™•ç†
    dfq = df[question_cols].apply(pd.to_numeric, errors="coerce").dropna()
    n_questions = len(question_cols)
    n_samples = dfq.shape[0]
    st.write(f"âœ”ï¸ æœ‰æ•ˆæ¨£æœ¬æ•¸ï¼š{n_samples}ï¼Œé¡Œç›®æ•¸ï¼š{n_questions}")

    if n_samples < 5:
        st.warning("æ¨£æœ¬æ•¸éå°‘ï¼Œç„¡æ³•é€²è¡Œå¾ŒçºŒåˆ†æã€‚")
        st.stop()

    user_n_factors = st.number_input("é æœŸå› å­æ•¸", min_value=1, max_value=n_questions, value=2)

    # ===== Step 2: KMO/Bartlett =====
    st.header("Step 2. å› å­åˆ†æé©é…æ€§æª¢å®š (KMO / Bartlettâ€™s Test)")
    try:
        kmo_all, kmo_model = calculate_kmo(dfq)
        bartlett_chi2, bartlett_p = calculate_bartlett_sphericity(dfq)
        st.write(f"KMO æŒ‡æ•¸: **{kmo_model:.2f}**")
        st.write(f"Bartlettâ€™s æª¢å®š p-value: **{bartlett_p:.4f}**")
        if kmo_model < 0.6:
            st.warning("KMO æŒ‡æ•¸ < 0.6ï¼Œä¸å»ºè­°é€²è¡Œå› å­åˆ†æï¼")
        if bartlett_p >= 0.05:
            st.warning("Bartlettâ€™s p-value >= 0.05ï¼Œè³‡æ–™ç„¡æ³•è­‰æ˜å…·å› å­çµæ§‹ï¼")
        if (kmo_model >= 0.6) and (bartlett_p < 0.05):
            st.success("âœ”ï¸ é€šé KMO èˆ‡ Bartlettâ€™s Testï¼Œé©åˆå› å­åˆ†æã€‚")
    except Exception as e:
        st.error(f"KMO/Bartlett æª¢å®šç„¡æ³•è¨ˆç®—ï¼Œè«‹ç¢ºèªè³‡æ–™æ ¼å¼ï¼ˆéŒ¯èª¤è¨Šæ¯ï¼š{e}ï¼‰")
        st.stop()

    # ===== Step 2.5: Scree Plot, Eigenvalue, ç´¯ç©è®Šç•°é‡ =====
    st.header("Step 2.5. å»ºè­°æœ€é©å› å­æ•¸")
    try:
        fa = FactorAnalyzer()
        fa.fit(dfq)
        ev, v = fa.get_eigenvalues()
        fig, ax = plt.subplots()
        sns.lineplot(x=np.arange(1, len(ev)+1), y=ev, marker='o', ax=ax)
        ax.axhline(1, color='r', ls='--', label='Eigenvalue=1')
        ax.set_title('Scree Plot')
        ax.set_xlabel('å› å­æ•¸')
        ax.set_ylabel('Eigenvalue')
        ax.legend()
        st.pyplot(fig)

        st.write("å„å› å­çš„ Eigenvalueï¼š", [f"{x:.2f}" for x in ev])
        cumulative_var = np.cumsum(fa.get_factor_variance()[1])
        st.write("ç´¯ç©è§£é‡‹è®Šç•°é‡ï¼š", [f"{x:.2%}" for x in cumulative_var])

        elbow = np.argmax(ev < 1) + 1
        suggested_n_factors = max(elbow, np.argmax(cumulative_var >= 0.6)+1)
        st.info(f"å»ºè­°å› å­æ•¸ï¼š{suggested_n_factors}ï¼ˆElbow/Eigenvalue>1/ç´¯ç©è®Šç•°é‡>60% ç¶œåˆåˆ¤æ–·ï¼‰")

        use_suggested = st.radio("è¦æ¡ç”¨å»ºè­°å› å­æ•¸å—ï¼Ÿ", ["æ¡ç”¨å»ºè­°", "ä¿æŒåŸè¨­å®š"])
        final_n_factors = suggested_n_factors if use_suggested == "æ¡ç”¨å»ºè­°" else user_n_factors
    except Exception as e:
        st.error(f"å› å­æ•¸å»ºè­°è¨ˆç®—å¤±æ•—ï¼š{e}")
        st.stop()

    # ===== Step 3: åŸ·è¡Œå› å­åˆ†æ =====
    st.header("Step 3. åŸ·è¡Œå› å­åˆ†æ")
    try:
        fa = FactorAnalyzer(n_factors=int(final_n_factors), rotation="varimax")
        fa.fit(dfq)
        loadings = pd.DataFrame(fa.loadings_, index=question_cols, columns=[f"å› å­{i+1}" for i in range(int(final_n_factors))])
        st.write("é¡Œç›®å› å­è¼‰è· (Loadings)ï¼š")
        st.dataframe(loadings.style.background_gradient(cmap="YlGnBu"), hide_index=True)
    except Exception as e:
        st.error(f"å› å­åˆ†æåŸ·è¡Œå¤±æ•—ï¼Œè«‹æª¢æŸ¥è³‡æ–™ï¼ˆéŒ¯èª¤è¨Šæ¯ï¼š{e}ï¼‰")
        st.stop()

    # æ¨™è¨˜ç•°å¸¸é¡Œé …
    issues = []
    for idx, row in loadings.iterrows():
        primary = row.abs().max()
        cross = sum(row.abs() > 0.4)
        if primary < 0.3:
            issues.append((idx, "è¼‰è·å€¼ä½ (<0.3)"))
        if cross > 1:
            issues.append((idx, "è·¨å› å­è¼‰è· > 0.4"))
    if issues:
        st.warning("âš ï¸ ä»¥ä¸‹é¡Œç›®è¼‰è·ç•°å¸¸ï¼Œå»ºè­°äººå·¥æª¢æŸ¥ï¼š")
        for idx, reason in issues:
            st.write(f"- {idx}ï¼š{reason}")
    else:
        st.success("æ‰€æœ‰é¡Œç›®è¼‰è·è¡¨ç¾æ­£å¸¸ï¼")

    # ===== Step 4: Cronbachâ€™s Î± =====
    st.header("Step 4. ä¸€è‡´æ€§æª¢é©—ï¼ˆCronbachâ€™s Î±ï¼‰")
    for i in range(int(final_n_factors)):
        items = loadings.index[loadings.iloc[:, i].abs() >= 0.3].tolist()
        if len(items) >= 2:
            subscale = dfq[items]
            corr = subscale.corr().values
            n = subscale.shape[1]
            avg_inter_corr = (np.sum(corr)-n)/(n*(n-1))
            alpha_val = n*avg_inter_corr / (1+(n-1)*avg_inter_corr)
            st.write(f"å› å­{i+1} Cronbachâ€™s Î±ï¼š**{alpha_val:.2f}** ï¼ˆ{', '.join(items)}ï¼‰")
        else:
            st.write(f"å› å­{i+1} Cronbachâ€™s Î± ç„¡æ³•è¨ˆç®—ï¼ˆæœ‰æ•ˆé¡Œç›®æ•¸ä¸è¶³ï¼‰")

    # ===== Step 5 & 6: ç•°å¸¸å¡«ç­”è‡ªå‹•æ¨¡å‹é¸æ“‡ =====
    st.header("Step 5. ç•°å¸¸å¡«ç­”æª¢æ¸¬æ¨¡å‹è‡ªå‹•é¸æ“‡")
    model_used = None
    anomaly_score = None
    threshold = None
    if (n_questions > 10) and (n_samples > 100):
        model_used = "Autoencoder (éœ€å¦è¡Œå®¢è£½å¯¦ä½œ)"
        st.warning("Autoencoder å±¬æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œéœ€å®¢è£½ï¼Œé€™è£¡é è¨­ç”¨ Isolation Forest ç¤ºç¯„")
        # é è¨­ç”¨ Isolation Forest ä»¥æ–¹ä¾¿æ¸¬è©¦
    elif (5 <= n_questions <= 20) and (30 <= n_samples <= 100):
        model_used = "Isolation Forest"
    elif n_samples < 30:
        model_used = None
        st.info("æ¨£æœ¬éå°‘ï¼Œä¸é€²è¡Œç•°å¸¸å¡«ç­”æª¢æ¸¬")
    else:
        model_used = "Isolation Forest"
    if model_used == "Isolation Forest":
        X = StandardScaler().fit_transform(dfq)
        iso = IsolationForest(random_state=42)
        preds = iso.fit_predict(X)
        anomaly_score = -iso.decision_function(X)
        st.success("å·²è‡ªå‹•é¸ç”¨ Isolation Forest ç•°å¸¸æª¢æ¸¬æ¨¡å‹")
    elif model_used == "Autoencoder (éœ€å¦è¡Œå®¢è£½å¯¦ä½œ)":
        st.info("Autoencoder æ¨¡å‹æœªé è¨­å¯¦ä½œï¼Œéœ€è‡ªè¨‚ã€‚")
    if anomaly_score is not None:
        st.write("å„æ¨£æœ¬ç•°å¸¸åˆ†æ•¸ï¼ˆè¶Šé«˜è¶Šç•°å¸¸ï¼‰ï¼š")
        st.dataframe(pd.DataFrame({"ç•°å¸¸åˆ†æ•¸": anomaly_score}), hide_index=True)
        threshold = np.percentile(anomaly_score, 95)
        st.write(f"å»ºè­°ç•°å¸¸é–€æª»ï¼ˆ95ç™¾åˆ†ä½ï¼‰ï¼š**{threshold:.2f}**")
        anomaly_flag = anomaly_score > threshold

        # ç›´æ–¹åœ–
        fig2, ax2 = plt.subplots()
        sns.histplot(anomaly_score, bins=20, ax=ax2, color="blue")
        ax2.axvline(threshold, color="red", linestyle="--", label="ç•°å¸¸é–€æª»")
        ax2.set_title("ç•°å¸¸åˆ†æ•¸åˆ†å¸ƒ")
        ax2.legend()
        st.pyplot(fig2)

        # ===== Step 7: é™ç¶­è¦–è¦ºåŒ– (UMAP) =====
        st.header("Step 6. é™ç¶­è¦–è¦ºåŒ–èˆ‡ç•°å¸¸æ¨£æœ¬æ¨™è¨˜ (UMAP)")
        reducer = umap.UMAP(random_state=42)
        embedding = reducer.fit_transform(X)
        plt.figure(figsize=(6,4))
        plt.scatter(embedding[:,0], embedding[:,1], c=anomaly_flag, cmap="coolwarm", s=40, alpha=0.8)
        plt.title("å¡«ç­”æ¨£æœ¬é™ç¶­è¦–è¦ºåŒ–ï¼ˆç´…=ç–‘ä¼¼ç•°å¸¸ï¼‰")
        st.pyplot(plt)
        st.write("é»é¸è¡¨æ ¼å¯é€²ä¸€æ­¥æŸ¥çœ‹ç–‘ä¼¼ç•°å¸¸å¡«ç­”è€…åŸå§‹è³‡æ–™ã€‚")
else:
    st.info("è«‹å…ˆä¸Šå‚³å•å·è³‡æ–™æª”æ¡ˆã€‚")
